{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Glove_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBKhwXa-cIvw"
      },
      "source": [
        "!pip install stanza\n",
        "import stanza\n",
        "import codecs\n",
        "import random\n",
        "import pickle\n",
        "import math\n",
        "import numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAxVRicLafVz",
        "outputId": "0d6bb06b-bd34-4c04-bee3-857f9fe95fab"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIEkSbLtddFD",
        "outputId": "3b4691e9-c778-4b5c-9043-9e116f83db61"
      },
      "source": [
        "HindiTextTotal = codecs.open(\"/content/drive/My Drive/SMDM/hi_wiki.txt\",'r','utf-8').read().split('\\n')\n",
        "HindiTextNonEmpty = list(filter(None, HindiTextTotal))\n",
        "print(\"Hindi :\",len(HindiTextNonEmpty))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hindi : 115378\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw6u22BQfPvg"
      },
      "source": [
        "stanza.download('hi')\n",
        "nlpHi = stanza.Pipeline(lang='hi', processors='tokenize')\n",
        "lineNo = 1;\n",
        "Sentences = []\n",
        "for line in HindiTextNonEmpty:\n",
        "  doc = nlpHi(line)\n",
        "  for sent in doc.sentences:\n",
        "    tokens = []\n",
        "    for token in sent.tokens:\n",
        "      tokens.append(token.text)\n",
        "    Sentences.append(tokens)\n",
        "  if (lineNo % 1000 == 0):\n",
        "    print(lineNo)\n",
        "  lineNo = lineNo + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN5l5Cug3Ru5",
        "outputId": "6196ce67-5004-4cd3-91ff-4dab2072f26e"
      },
      "source": [
        "random.shuffle(Sentences)\n",
        "print(len(Sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "385970\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfYW6fkH3hkj",
        "outputId": "382be9aa-f177-47d5-c149-b451d3465e2a"
      },
      "source": [
        "print(\"Training Set Size: \", len(Sentences))\n",
        "with open(\"/content/drive/My Drive/SMDM/TotalTrainSet.txt\", \"wb\") as fp:  \n",
        "  pickle.dump(Sentences, fp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Set Size:  385970\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YDPshXdZjvf",
        "outputId": "98d128e4-8e2f-4260-bef2-14b940d147e3"
      },
      "source": [
        "with open(\"/content/drive/My Drive/SMDM/TotalTrainSet.txt\", \"rb\") as fp:\n",
        "  TrainSet = pickle.load(fp)\n",
        "print(len(TrainSet))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "385970\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9v3hP2Ga_YL"
      },
      "source": [
        "windowSize = 5\n",
        "def GetCoocurrenceMatrixAndVocabulary(Set):\n",
        "  cooccur = {}\n",
        "  vocabulary = []\n",
        "\n",
        "  count = 0\n",
        "\n",
        "  # Iterate over all the sentences\n",
        "  for sent in Set:\n",
        "    count += 1\n",
        "    if count%10000==0:\n",
        "      print(\"Reached Sentence:\",count)\n",
        "    for i in range(len(sent)):\n",
        "      tok = sent[i]\n",
        "      # If token not in the vocabulary, then add it\n",
        "      if tok not in vocabulary:\n",
        "        vocabulary.append(tok)\n",
        "      \n",
        "      jmin = max(0,i-windowSize)\n",
        "      jmax = min(len(sent),i+windowSize+1)\n",
        "\n",
        "      for j in range(jmin,jmax):\n",
        "        if i == j:\n",
        "          continue\n",
        "        context = sent[j]\n",
        "        val = (tok,context)\n",
        "        if val not in cooccur:\n",
        "          cooccur[val] = 0\n",
        "        # Increment the count of the coocurrence for the val\n",
        "        cooccur[val] = cooccur[val]+1\n",
        "    \n",
        "  return (cooccur,vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B42qSWzRcOjM",
        "outputId": "edc30f1b-8e30-4717-84e5-0a90341d0140"
      },
      "source": [
        "res = GetCoocurrenceMatrixAndVocabulary(TrainSet)\n",
        "cooccur = res[0]\n",
        "vocabulary = res[1]\n",
        "\n",
        "print(\"Size of the vocabulary:\",len(vocabulary))\n",
        "print(\"Size of the cooccurrenceMatrix:\",len(cooccur))\n",
        "\n",
        "with open(\"/content/drive/My Drive/SMDM/VocabularyGloVe.txt\", \"wb\") as fp:\n",
        "  pickle.dump(vocabulary, fp)\n",
        "\n",
        "with open(\"/content/drive/My Drive/SMDM/CoocurranceMatrix.txt\", \"wb\") as fp:\n",
        "  pickle.dump(cooccur, fp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reached Sentence: 10000\n",
            "Reached Sentence: 20000\n",
            "Reached Sentence: 30000\n",
            "Reached Sentence: 40000\n",
            "Reached Sentence: 50000\n",
            "Reached Sentence: 60000\n",
            "Reached Sentence: 70000\n",
            "Reached Sentence: 80000\n",
            "Reached Sentence: 90000\n",
            "Reached Sentence: 100000\n",
            "Reached Sentence: 110000\n",
            "Reached Sentence: 120000\n",
            "Reached Sentence: 130000\n",
            "Reached Sentence: 140000\n",
            "Reached Sentence: 150000\n",
            "Reached Sentence: 160000\n",
            "Reached Sentence: 170000\n",
            "Reached Sentence: 180000\n",
            "Reached Sentence: 190000\n",
            "Reached Sentence: 200000\n",
            "Reached Sentence: 210000\n",
            "Reached Sentence: 220000\n",
            "Reached Sentence: 230000\n",
            "Reached Sentence: 240000\n",
            "Reached Sentence: 250000\n",
            "Reached Sentence: 260000\n",
            "Reached Sentence: 270000\n",
            "Reached Sentence: 280000\n",
            "Reached Sentence: 290000\n",
            "Reached Sentence: 300000\n",
            "Reached Sentence: 310000\n",
            "Reached Sentence: 320000\n",
            "Reached Sentence: 330000\n",
            "Reached Sentence: 340000\n",
            "Reached Sentence: 350000\n",
            "Reached Sentence: 360000\n",
            "Reached Sentence: 370000\n",
            "Reached Sentence: 380000\n",
            "Size of the vocabulary: 336077\n",
            "Size of the cooccurrenceMatrix: 18393266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0beCKO4uxda"
      },
      "source": [
        "with open(\"/content/drive/My Drive/SMDM/VocabularyGloVe.txt\", \"rb\") as fp:\n",
        "  vocabulary = pickle.load(fp)\n",
        "\n",
        "with open(\"/content/drive/My Drive/SMDM/CoocurranceMatrix.txt\", \"rb\") as fp:\n",
        "  cooccur = pickle.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqywe7jBvev1"
      },
      "source": [
        "alpha = 0.75\n",
        "xmax = 100\n",
        "vectorSize = 100\n",
        "\n",
        "def f(x):\n",
        "  if x < xmax:\n",
        "    val = x/xmax\n",
        "    ans = math.pow(val,alpha)\n",
        "    return ans\n",
        "  else:\n",
        "    return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnihoVJNwh1x"
      },
      "source": [
        "# Initialize \n",
        "biasWord = {}\n",
        "biasContext = {}\n",
        "embeddingWord = {}\n",
        "embeddingContext = {}\n",
        "\n",
        "def InitializeVectors(vocabulary):\n",
        "  embeddingWord.clear()\n",
        "  embeddingContext.clear()\n",
        "  biasContext.clear()\n",
        "  biasWord.clear()\n",
        "\n",
        "  for tok in vocabulary:\n",
        "    embeddingWord[tok] = numpy.random.random(vectorSize)\n",
        "    embeddingContext[tok] = numpy.random.random(vectorSize)\n",
        "    biasWord[tok] = random.random()\n",
        "    biasContext[tok] = random.random()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw2UykXh8YOi"
      },
      "source": [
        "numberOfIterations = 50\n",
        "\n",
        "def trainModel():\n",
        "  learningRate = 0.0005\n",
        "  # Initialize the vocabulary\n",
        "  InitializeVectors(vocabulary)\n",
        "  print(\"Learning Started\")\n",
        "  prevCost = math.inf\n",
        "  # run gradient descent for numberOfIterations\n",
        "  for iterationCount in range(numberOfIterations):\n",
        "    # Initialize cost\n",
        "    cost = 0\n",
        "    # Iterate over the cooccurance matrix\n",
        "    for val in cooccur:\n",
        "      token = val[0]\n",
        "      context = val[1]\n",
        "      count = cooccur[val]\n",
        "\n",
        "      if token == context:\n",
        "        continue\n",
        "      # Find the function value\n",
        "      fval = f(count)\n",
        "      # Intermediate Loss is the second term in the loss function\n",
        "      intermediateLoss = numpy.dot(embeddingWord[token],embeddingContext[context]) + biasWord[token] + biasContext[context] - math.log(count)\n",
        "      # print(token,context,numpy.dot(embeddingWord[token],embeddingContext[context]),intermediateLoss)\n",
        "      # Update the cost\n",
        "      cost += math.pow(intermediateLoss,2) * fval\n",
        "\n",
        "      # Find the gradients\n",
        "      gradientEmbeddingWord = 2 * fval * intermediateLoss * embeddingContext[context]\n",
        "      gradientEmbeddingContext = 2 * fval * intermediateLoss * embeddingWord[token]\n",
        "      gradientBias = 2 * fval * intermediateLoss\n",
        "\n",
        "      # Update the vectors\n",
        "      embeddingWord[token] -= learningRate * gradientEmbeddingWord\n",
        "      embeddingContext[token] -= learningRate * gradientEmbeddingContext\n",
        "      # Update the constants\n",
        "      biasWord[token] -= learningRate * gradientBias\n",
        "      biasContext[token] -= learningRate * gradientBias\n",
        "\n",
        "    print(\"Iteration\",iterationCount+1,\"Complete. Cost after it:-\", cost)\n",
        "\n",
        "    # If cost has increased then the learning rate must be reduced\n",
        "    if cost > prevCost:\n",
        "      learningRate= learningRate/2\n",
        "    prevCost = cost\n",
        "\n",
        "  print(\"Training Complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrsAspjtGVko",
        "outputId": "8c20bcbc-9f2f-489e-abbc-62aa3588e330"
      },
      "source": [
        "trainModel()\n",
        "with open(\"/content/drive/My Drive/SMDM/EmbeddingWord.txt\", \"wb\") as fp:\n",
        "  pickle.dump(embeddingWord, fp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning Started\n",
            "Iteration 1 Complete. Cost after it:- 123939985.77611367\n",
            "Iteration 2 Complete. Cost after it:- 117903460.77235915\n",
            "Iteration 3 Complete. Cost after it:- 105684728.82918566\n",
            "Iteration 4 Complete. Cost after it:- 91719616.09895669\n",
            "Iteration 5 Complete. Cost after it:- 83711414.45911822\n",
            "Iteration 6 Complete. Cost after it:- 79387158.17370589\n",
            "Iteration 7 Complete. Cost after it:- 76942769.62194523\n",
            "Iteration 8 Complete. Cost after it:- 75488090.53121583\n",
            "Iteration 9 Complete. Cost after it:- 74596717.45495622\n",
            "Iteration 10 Complete. Cost after it:- 74050372.12846915\n",
            "Iteration 11 Complete. Cost after it:- 73725273.9901387\n",
            "Iteration 12 Complete. Cost after it:- 73543105.15423363\n",
            "Iteration 13 Complete. Cost after it:- 73449369.04362631\n",
            "Iteration 14 Complete. Cost after it:- 73403428.99103324\n",
            "Iteration 15 Complete. Cost after it:- 73373659.24956337\n",
            "Iteration 16 Complete. Cost after it:- 73334906.14392161\n",
            "Iteration 17 Complete. Cost after it:- 73267030.23763233\n",
            "Iteration 18 Complete. Cost after it:- 73153976.43394151\n",
            "Iteration 19 Complete. Cost after it:- 72983117.69676958\n",
            "Iteration 20 Complete. Cost after it:- 72744752.89187579\n",
            "Iteration 21 Complete. Cost after it:- 72431700.71487696\n",
            "Iteration 22 Complete. Cost after it:- 72038959.76187488\n",
            "Iteration 23 Complete. Cost after it:- 71563417.67912693\n",
            "Iteration 24 Complete. Cost after it:- 71003598.40992668\n",
            "Iteration 25 Complete. Cost after it:- 70359439.6968601\n",
            "Iteration 26 Complete. Cost after it:- 69632094.91346952\n",
            "Iteration 27 Complete. Cost after it:- 68823754.69927965\n",
            "Iteration 28 Complete. Cost after it:- 67937485.01330349\n",
            "Iteration 29 Complete. Cost after it:- 66977079.17584774\n",
            "Iteration 30 Complete. Cost after it:- 65946922.24668611\n",
            "Iteration 31 Complete. Cost after it:- 64851866.686305106\n",
            "Iteration 32 Complete. Cost after it:- 63697118.67649617\n",
            "Iteration 33 Complete. Cost after it:- 62488134.75033428\n",
            "Iteration 34 Complete. Cost after it:- 61230528.5282922\n",
            "Iteration 35 Complete. Cost after it:- 59929987.40409438\n",
            "Iteration 36 Complete. Cost after it:- 58592199.0000221\n",
            "Iteration 37 Complete. Cost after it:- 57222787.14421015\n",
            "Iteration 38 Complete. Cost after it:- 55827257.03225385\n",
            "Iteration 39 Complete. Cost after it:- 54410949.14089571\n",
            "Iteration 40 Complete. Cost after it:- 52979001.373171404\n",
            "Iteration 41 Complete. Cost after it:- 51536318.84203726\n",
            "Iteration 42 Complete. Cost after it:- 50087550.64490441\n",
            "Iteration 43 Complete. Cost after it:- 48637072.9483843\n",
            "Iteration 44 Complete. Cost after it:- 47188977.689142086\n",
            "Iteration 45 Complete. Cost after it:- 45747066.20045119\n",
            "Iteration 46 Complete. Cost after it:- 44314847.094595484\n",
            "Iteration 47 Complete. Cost after it:- 42895537.76263514\n",
            "Iteration 48 Complete. Cost after it:- 41492068.89452254\n",
            "Iteration 49 Complete. Cost after it:- 40107091.47044498\n",
            "Iteration 50 Complete. Cost after it:- 38742985.725793205\n",
            "Training Complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmDArLt4b4ED"
      },
      "source": [
        "with open(\"/content/drive/My Drive/SMDM/EmbeddingWord.txt\", \"rb\") as fp:\n",
        "  embeddingWord = pickle.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WL6QKHkHcXo8"
      },
      "source": [
        "totOutput = \"\"\n",
        "for val in embeddingWord.keys():\n",
        "  outputLine = val\n",
        "  outputLine += \" \"\n",
        "  outputLine += \" \".join(map(str,embeddingWord[val]))\n",
        "  totOutput += outputLine + \"\\n\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jp9Ys2oeRKQ"
      },
      "source": [
        "file1 = open(\"/content/drive/My Drive/gloveHindi.txt\",\"w\")\n",
        "file1.write(totOutput)\n",
        "file1.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}